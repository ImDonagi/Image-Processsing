{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oslnTmQ94FMx",
        "tTFEqdjU48SL",
        "zvKyFZ-pX4UD",
        "WOsmSLP88jj0",
        "8TbFuTdEYygU",
        "Ql5IYx7cZtZB",
        "4Myl7-snaZF7",
        "zdhkLMSVa0wS",
        "_qKI25A7c2N2",
        "Q_3x0_34eKOM",
        "GA05OEwLhPbo",
        "yFslnWT22TFo",
        "h0If2-lO6C_i",
        "y1IDZlyd8T5l",
        "feVGhy1Y9Fv0",
        "c-WHT2yP97cM",
        "S7eMsDBpLYyG",
        "VljIiMtjMH2B",
        "_dFgbRaoNmaQ",
        "PSDdtrSPP3KI",
        "TQNeQm8e7-04",
        "dDDtIP4O9f5m",
        "ZlE36MWG_iwW",
        "5YQSudCdAf0W",
        "m9PzMu4vEnSj",
        "2dVBIKIgFUy8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Course Notes- Guy Donagi**"
      ],
      "metadata": {
        "id": "-zySApGk2-QD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "4-D9B1qK3HL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Week 1\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Week 2\n",
        "import glob\n",
        "from skimage import img_as_float\n",
        "\n",
        "# Week 3\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Week 4\n",
        "import scipy.ndimage as ndi\n",
        "from skimage import data # Is used for showing stock photos from skimage library\n",
        "from skimage import filters # And more specifically:\n",
        "from skimage.filters import unsharp_mask\n",
        "from skimage.filters import gaussian\n",
        "from skimage.morphology import disk # Structuring element\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
        "\n",
        "# Week 5\n",
        "from skimage.filters import threshold_multiotsu\n",
        "from skimage.filters.rank import entropy\n",
        "from skimage.morphology import disk\n",
        "\n",
        "# Week 6\n",
        "from scipy import ndimage\n",
        "from skimage.feature import peak_local_max\n",
        "from skimage.segmentation import watershed\n",
        "import imutils\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "\n",
        "#  Aruco\n",
        "from skimage import measure, io, img_as_ubyte, morphology, util, color\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import label2rgb, rgb2gray\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import imutils\n",
        "\n",
        "#  ML\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ArYVFDzO3xkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A function to display an image.\n",
        "First, you will need to define a path for the image."
      ],
      "metadata": {
        "id": "oslnTmQ94FMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display(img): # Displays the image\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3oLPl7UK4Co_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/some/path/img.jpg'\n",
        "\n",
        "# Reading the image\n",
        "img = io.imread(img_path)"
      ],
      "metadata": {
        "id": "uXC05Wbv4N1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the nparray.shape / .reshape attribute\n",
        "It is used to extract the image's height/width/number of channels."
      ],
      "metadata": {
        "id": "tTFEqdjU48SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([[1, 2, 3, 4],\n",
        "                [5, 6, 7, 8],\n",
        "                [9,10,11,12]])\n",
        "\n",
        "print(arr.shape) # Will print a tuple consists of rows, columns, and depth (if it exists).\n",
        "print(arr[1])# Printing the second row\n",
        "arr = arr.reshape((1,12)) # Reshaping an array, NOTICE THE DOUBLE PARENTHESIS\n",
        "print(arr)\n",
        "print()\n",
        "\n",
        "# Example\n",
        "img = np.zeros((30,40,3))\n",
        "height, width = img.shape[:2] # Assigning height and width with the shape attribute (for a three channeled image).\n",
        "height = int(height) # Converting to int.\n",
        "width = int(width) # Converting to int. \n",
        "print(\"The image's height is\", height ,\"px.\")\n",
        "print(\"The image's width is\", width ,\"px.\")\n",
        "\n",
        "number_of_channels = img.shape[2] # Getting the amount of channels using the shape attribute\n",
        "print(\"The image's bit depth is\",number_of_channels, \"channels.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy2RETJz5MV3",
        "outputId": "bf398499-cf33-45b1-f947-8c411817459e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "[5 6 7 8]\n",
            "[[ 1  2  3  4  5  6  7  8  9 10 11 12]]\n",
            "\n",
            "The image's height is 30 px.\n",
            "The image's width is 40 px.\n",
            "The image's bit depth is 3 channels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the depth of an image\n",
        "Depth - How many bits does it take to represent the colors in the given image."
      ],
      "metadata": {
        "id": "zvKyFZ-pX4UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(img.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI_YOM1UX-Xs",
        "outputId": "bc18aa22-fe61-4f12-9bb1-06e70236e7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cropping an image"
      ],
      "metadata": {
        "id": "WOsmSLP88jj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "specificRegion = img[ :1100,:] # Cropping the pixels form the top until the \n",
        "display(specificRegion)"
      ],
      "metadata": {
        "id": "QUdjyp5o-CjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a copy of an image"
      ],
      "metadata": {
        "id": "8TbFuTdEYygU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_copy = img.copy() # We need to make copies of our images in order to manipulate them."
      ],
      "metadata": {
        "id": "LV39RZWlY1hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting a certain channel from an image"
      ],
      "metadata": {
        "id": "Ql5IYx7cZtZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_copy = img.copy() # Making a copy of the original image in order to make my changes on it\n",
        "redOnly = img[:,:,0] # Assigning a new value to the copy, consisting of the red channel only"
      ],
      "metadata": {
        "id": "akMUe3HNZ1Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving an image to a desired path"
      ],
      "metadata": {
        "id": "4Myl7-snaZF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "io.imsave(fname = '/some/path/redOnly.jpg', arr=redOnly) # Saving an image"
      ],
      "metadata": {
        "id": "mRSI7wmfaeHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matrix operations with numpy"
      ],
      "metadata": {
        "id": "zdhkLMSVa0wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.transpose(img) # Transposing an image\n",
        "# Dot product. Note that the number of rows in the first image needs to match the number of columns of the second image.\n",
        "myDotProduct = np.dot(someChannel, np.transpose(otherChannel))\n",
        "\n",
        "img.sum() # Sums our matrix\n",
        "img.mean() # Calculating the mean of the matrix\n",
        "img.std() # Calculating the stndard deviation of the matrix\n",
        "\n",
        "np.subtract(array1, array2) # Subtracting array2 from array1\n",
        "np.add(array1, array2) # Adding the two arrays together\n",
        "np.divide(numerator,denominator) # Division: numerator/denominator\n"
      ],
      "metadata": {
        "id": "eXJO28u7a5fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Boolean masks\n",
        "A boolean mask is created when you compare two arrays."
      ],
      "metadata": {
        "id": "_qKI25A7c2N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For examples:\n",
        "\n",
        "matrix = 2*np.random.rand(5, 5) - 1\n",
        "print(matrix)\n",
        "mask = (matrix < 0)\n",
        "print()\n",
        "print(mask)\n",
        "matrix[mask] = 0\n",
        "print()\n",
        "print(matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9KBJ8rgc5w-",
        "outputId": "a056d167-ab69-48a7-b04d-24f63539cecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.32390479 -0.62865096  0.83378989  0.87775511  0.34752133]\n",
            " [ 0.26453513  0.41502695 -0.50371255 -0.0852176   0.48174719]\n",
            " [ 0.80078145 -0.1507299  -0.61318047 -0.24150801  0.83689713]\n",
            " [-0.30780392  0.77893788  0.41481159 -0.61106775  0.25903836]\n",
            " [ 0.40825311 -0.15830142 -0.86314735  0.27135472 -0.72284789]]\n",
            "\n",
            "[[False  True False False False]\n",
            " [False False  True  True False]\n",
            " [False  True  True  True False]\n",
            " [ True False False  True False]\n",
            " [False  True  True False  True]]\n",
            "\n",
            "[[0.32390479 0.         0.83378989 0.87775511 0.34752133]\n",
            " [0.26453513 0.41502695 0.         0.         0.48174719]\n",
            " [0.80078145 0.         0.         0.         0.83689713]\n",
            " [0.         0.77893788 0.41481159 0.         0.25903836]\n",
            " [0.40825311 0.         0.         0.27135472 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalizing an image\n",
        "* Used in order to change the range of pixel intensity values.\n",
        "* Can be substituted with reading the image as float. "
      ],
      "metadata": {
        "id": "Q_3x0_34eKOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function for normalizing the RGB pictures:\n",
        "def normalize(img):\n",
        "  numerator = img - np.min(img)\n",
        "  denominator = np.max(img) - np.min(img)\n",
        "  return numerator / denominator\n",
        "\n",
        "normalize(img)\n",
        "# Can be subtituted with\n",
        "new_img = img_as_float(img)"
      ],
      "metadata": {
        "id": "XUmQC0p8hMJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading all of the images from a certain folder"
      ],
      "metadata": {
        "id": "GA05OEwLhPbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A list that will hold all of the paths from the desired folder\n",
        "pathList = glob.glob(f'{folder_path}/Images/*.*')\n",
        "\n",
        "imageList=[]  # A list that will store our images\n",
        "for path in pathList:\n",
        "  img = io.imread(path) # Reading the image from every path in the list\n",
        "  imageList.append(img) # Appending the image to the image list"
      ],
      "metadata": {
        "id": "H7WmkWPTnPbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Writing an index in a scientific and prettier way:\n",
        "> $$MGVRI = \\frac{(GREEN^2 - RED^2)}{(GREEN^2 + RED^2)}$$"
      ],
      "metadata": {
        "id": "90E9hKuan0yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How to use subplots"
      ],
      "metadata": {
        "id": "yFslnWT22TFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bright_square = np.zeros((7, 7), dtype=float) # create a zero matrix\n",
        "bright_square[2:5, 2:5] = 1 # change center values to 1\n",
        "\n",
        "# Can be created manually or atomatically with a for loop\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.subplot(1,2,1, title=\"First\")\n",
        "plt.imshow(bright_square, cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(1,2,2, title=\"Second\")\n",
        "plt.imshow(bright_square)\n",
        "plt.colorbar()\n",
        "\n",
        "# Or it can be created with a for loop\n",
        "fig, ax=plt.subplots(ncols=1, nrows=2, figsize=(8, 8))\n",
        "plt.tight_layout() # nicer layout\n",
        "\n",
        "# loop to plt.imshow all the images in one row\n",
        "for i in range(0,2):\n",
        "  ax[i].imshow(bright_square)\n",
        "  ax[i].set_title(f'Image {i+1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BcsFY7bj2Y7w",
        "outputId": "3b794292-5e79-46ff-cfd9-096ffdcd15bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAGUCAYAAAB6NlaYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeQ0lEQVR4nO3dfdCld13f8feHzZMJSagEKcmuJC2BNtIKzDboYBVE6hKVOIU6CWgFmW7baShWrBPUQZs+OLSjhc5ksCsgiEhKI9AduhJ5SikIMRvIhDwQ3W6h2QAmSwIENE+73/5xzs0c7uxm773v3zm/6zr3+zVzJvd52LPfk5nvfu7v7/pd50pVIUlSb4/pXYAkSWAgSZIGwkCSJA2CgSRJGgQDSZI0CAaSJGkQDCQtvSRvTXJXkpuP8nyS/Jck+5LclORZi65RGpN59ZSBpM3gbcCOR3n+hcD509tO4E0LqEkas7cxh54ykLT0qupjwD2P8pKLgd+riU8Bj0vypMVUJ43PvHrqhFYFSsdjx44ddfDgwSbvdcMNN9wC3D/z0K6q2nUcb3EOcMfM/QPTx77UoDxpIX70eafVV+451OS9brjpgS49ZSCpi4MHD7J3794m75Xk/qra3uTNpJH6yj2H+NNrvrvJe2150p936SkDSd0M6HsU7wS2zdzfOn1MGo0CDnO4dxkr1tVTHkOSYDfwj6c7g74P+FpVuVwnrd+6esoJSd0sakJK8i7gucBZSQ4AvwacOK3ht4E9wEXAPuAvgVcspDCpqeJQLWZCmldPGUjqZlGBVFWXHuP5Av7FQoqR5mSyZDfunnLJTpI0CE5I6qKqhrSpQVoKA9rUsC4GkroxkKR2iuLQyHvKJTtJ0iA4IakbJySprUVtapgXA0ndGEhSOwUcGnkguWQnSRoEJyR144QkteWSnbQObvuW2ipwl50kSS04IakbJySprXGfFmsgqSMDSWqnKHfZSZLUghOSunFCkhoqODTyljKQ1I2BJLUzufzEuLlkJ0kaBCckdeF5SFJr4RDpXcSGGEjqxkCS2ing8MhbyiU7SdIgOCGpGyckqS2X7KR1MpCkdiaXnxh3ILlkJ0kaBCckdeEuO6m9wzXuCclAUjcGktSOS3aSJDXihKRunJCkdopwaOQzhoGkbgwkqS2PIUnrZCBJ7XgMSZKkRpyQ1IXbvqXWwqEa94xhIKkbA0lqZ3I9pHEH0rirlyQtDSckdeOEJLU19k0NBpK6MZCkdqrGfwxp3NVLkpaGE5K6cUKS2jrskp10/Nz2LbU1OTF23Ite465ekrQ0nJDUjROS1NL4NzUYSOrGQJLa8cRYSZIacUJSN05IUluHvPyEtD4GktTOMlygb9zVS5KWhhOSuvA8JKm9w+6yk9bHQJLa8cRYSZIacUJSN05IUjtF3GUnrZeBJLXlibGSJDXghKRunJCkdqrwu+yk9XDbt9RaRn89pHHHqSRpaTghqRsnJKmdwiU7ad0MJKktT4yVJKkBJyR144QktVOEw54YK62PgSS15ZKdJEkNOCGpC89DktoqvPyEtG4GktRSOOSJsZIkbZwTkrpxQpLacclO2gADSWrLJTtJkhpwQlI3TkhSO1VxyU5aD7d9S+2N/ctVx129tEZJdiS5Pcm+JJcf4fnvTvLRJJ9JclOSi3rUKY3FPHrKCUndLGpCSrIFuBJ4AXAAuD7J7qq6deZlvwq8u6relOQCYA9w7kIKlBooWNgF+ubVUwaSulngkt2FwL6q2g+Q5CrgYmC2eQo4Y/rzmcAXF1Wc1EYWuWQ3l54ykLQMzkqyd+b+rqraNXP/HOCOmfsHgGeveo9fB/44yauA04AfmUeh0kh06SkDSd00nJAOVtX2Db7HpcDbquo3k3w/8I4kT6+qww3qk+ZucmJssyW7Lj1lIKmbBS7Z3Qlsm7m/dfrYrFcCOwCq6pNJTgHOAu5aSIVSAwu8/MRcespddtoMrgfOT3JekpOAS4Ddq17z/4DnAyT528ApwN0LrVIaj7n0lBOSuljkeUhV9XCSy4BrgC3AW6vqliRXAHurajfwGuB3kvwrJqsfLy9PlNKILPKKsfPqKQNJ3Szy3/uq2sNk2+nsY6+b+flW4DkLK0iag8MLXPSaR0+5ZCdJGgQnJHXjipjUThUcWtCS3bwYSOrGQJLaWtQxpHlxyU6SNAhOSOrGCUlqZ7LLbtwzhoGkLrz8hNTe2K8YayCpGwNJaqfxVwd1Me75TpK0NJyQ1I0TktSSx5CkdTOQpLYWdYG+eRl3nEqSloYTkrpxQpLa8ZsapHVy27fU3tiPIY27eknS0nBCUjdOSFI7i7we0rwYSOrGQJLacpedJEkNOCGpGyckqZ1l+OogA0ndGEhSW+6ykySpASckdeF5SFJj5S47ad0MJKmdwl12Sy/JN5L8jd51SFq7JC9P8vHedej4GEgzknw+yV9NQ+gbSb4BPLWq9q/jvZ6b5MAcylwaK8t2G72pjyQ/kORPknwtyT1JPpHk7/WuazM7PF222+itF5fsHuknqupDa3lhki1VdWjeBS0rw2S8kpwBvB/458C7gZOAvw880LOuzWwZtn07IR1DkkrylOnPb0vypiR7knwTeF6Si5LcmuS+JHcm+cUkpwF/BJw9M22d3fWDSG09FaCq3lVVh6rqr6rqj6vqJoAkP5fktiT3JrkmyZNX/mCS70nywelU9RdJfnn6+MlJ3pDki9PbG5KcPH3uuUkOJHlNkruSfCnJK2be8/FJdif5epI/Bf7mQv9vqAkD6fi9FPj3wOnAx4G3AP+0qk4Hng58pKq+CbwQ+GJVPXZ6+2K3igfKJbtR+zPgUJK3J3lhkr+28kSSi4FfBv4h8ATgfwPvmj53OvAh4APA2cBTgA9P/+ivAN8HPAP4XuBC4Fdn/s6/DpwJnAO8Erhy5u+9ErgfeBLwc9PbpjP2JTsD6ZHel+Sr09v7jvD8/6iqT1TV4aq6H3gIuCDJGVV1b1V9esH1jlKrMDKQ+qiqrwM/wGSl6HeAu6cTyhOBfwb8RlXdVlUPA/8BeMZ0Svpx4MtV9ZtVdX9V3VdV103f9mXAFVV1V1XdDfwb4Gdm/tqHps8/VFV7gG8AT0uyBXgx8Lqq+mZV3Qy8fe7/EwZm5ctVDaTl8pNV9bjp7SeP8Pwdq+6/GLgI+EKS/5Xk++dfotTfNHBeXlVbmawOnA28AXgy8MaVX+yAe4AwmWy2Af/nKG95NvCFmftfmD624ivTgFvxl8BjmUxhJ/DtvTn7PhoJA+n4fduv5FV1fVVdDHwX8D4mB3gf8To9khPS8qiqzwFvYxJMdzBZxn7czO07qupPps8d7TSKLzIJsxXfPX3sWO4GHmYSdrN/dtM5TJrcejGQNiDJSUleluTMqnoI+DpwePr0XwCPT3JmvwqHzUAaryR/a7rBYOv0/jbgUuBTwG8Dr03yPdPnzkzyj6Z/9P3Ak5L8/HQTw+lJnj197l3AryZ5QpKzgNcBv3+sWqY7Xd8D/HqSU5NcAPxsw487DuUxJE3WuD+f5OtM1s5fBt/6jfFdwP7p0oW77LRM7gOeDVw33XH6KeBm4DVV9V7g9cBV0764mckmH6rqPuAFwE8AXwb+HHje9D3/HbAXuAn4LPDp6WNrcRmT5bsvM5nUfndjH089eB7SjKo69wiPZebnl6967kFgx6O836bc6bNWTjfjVVV3Aj/1KM+/A3jHUZ67GXj+ER6/H/iX09vq564Ftq567NyZn+9msmFi01qG85AMJHVjIEltjT2QXLKTJA2CE5K6cEOC1NbKeUhjZiCpGwNJaqtGHkgu2UmSBuFRJ6Qk/gqrdak1/Kq2GSekk3JyncJpvcvQyNzPN3mwHjhmT439An0u2ambzRhIp3Aaz84jdjxLj+q6+vAxX1PlLjtJkppwQlI3m3FCkuZp7JsaDCR14bZvqbXxb/t2yU6SNAhOSOrGCUlqa+xLdk5I6mZZLj+RZEeS25PsS3J573q0Oa18uaqXn5A2qenls69kcnmFC4BLp9fjkXScXLJTN0OYbhq4ENhXVfsBklwFXAzc2rUqbT41ORdpzAwkdbMkgXQOk8tyrzjA5MJ135JkJ7AT4BROXVxl2nTG/k0NLtlJc1ZVu6pqe1VtP5GTe5cjDZYTkroYyoaEBu4Ets3c3zp9TFqoYvy77AwkdbMkgXQ9cH6S85gE0SXAS/uWpM1p/CfGGkjSBlTVw0kuA64BtgBvrapbOpcljZKBpG6WZEKiqvYAe3rXIY29pQwkdbMsgSQNxdiPIbnLTpI0CE5I6sYJSWqnavwTkoGkLpZo27c0GGPfZeeSnSRpEJyQ1I0TktTW2FvKQFI3BpLUlseQpHUykKR2iow+kDyGJEkaBCckdeEuO6m9sXeUgaRuDCSpoSU4D8klO2kDkrw1yV1Jbu5dizR2BpK6WVm22+its7cBO3oXIQHTiyI1uHViIKmbRQZSkh1Jbk+yL8nlR3nNTyW5NcktSf5gjZ/hY8A9a//U0vxUpcltLebRUx5D0tJLsgW4EngBcAC4Psnuqrp15jXnA68FnlNV9yb5roZ//05gJ8ApnNrqbaVu5tVTTkjqZoET0oXAvqraX1UPAlcBF696zT8Brqyqe6e13dXwc+6qqu1Vtf1ETm71ttIjTL5gdeO3NZhLTxlI6qJVGE0D6awke2duO1f9decAd8zcPzB9bNZTgacm+USSTyXxuJBGpWi6ZNelp1yy0zI4WFXbN/geJwDnA88FtgIfS/J3quqrGy1OGqEuPeWEpG4WuGR3J7Bt5v7W6WOzDgC7q+qhqvq/wJ8xaaZHleRdwCeBpyU5kOSVa/rwUmsFVNrcjm0uPWUgqZsFBtL1wPlJzktyEnAJsHvVa97H5Dc5kpzFZLlh/xo+w6VV9aSqOrGqtlbVW47n/4HU0gKPIc2lpwwkLb2qehi4DLgGuA14d1XdkuSKJC+avuwa4CtJbgU+CvzrqvpKn4qlYZtXT3kMSd0s8qTWqtoD7Fn12Otmfi7gF6Y3aZwWeFLrPHrKQFI3iwwkafl5+QlJkpoY/YS0GX/LTsb9WxB4+Ykhu+aLN/YuYeF+9Oxn9C6hjZG31OgDSeNlIEkNefkJSZLacEJSN05IUmMjbykDSd0YSFJrLtlJkrRhTkjqxglJamzkLWUgqYtl2fadZBvwe8ATmfxzsKuq3ti3Km1aI28pA0namIeB11TVp5OcDtyQ5IOzV86UtDYGkrpZhgmpqr4EfGn6831JbmNyoTIDSYu1cvmJETOQ1M0yBNKsJOcCzwSuW/X4TmAnwCmcuvC6tHmMvaXcZSc1kOSxwB8CP19VX599rqp2VdX2qtp+Iif3KVAaASckdbMsE1KSE5mE0Tur6j2969EmNvKWMpDUzTIEUibfdPsW4Laq+q3e9WiTG/kxJJfspI15DvAzwA8nuXF6u6h3UdIYOSGpi2U5D6mqPs7Yv69FSyMjbykDSd0sQyBJg1GM/hiSS3aSpEFwQlI3TkhSSxn9pgYDSd0YSFJjI28pl+wkSYPghKRunJCkxkbeUgaSuliWbd/SoIy8pVyykyQNghOSunFCkhry8hPS+hlIUltj/6YGl+wkSYPghKRulmFCSnIK8DHgZCb9dHVV/VrfqrRpjbylDCR1swyBBDwA/HBVfWN6XaSPJ/mjqvpU78KksTGQpA2oSap+Y3r3xOltKZJWWjQDSV0s03lISbYANwBPAa6squs6l6RNauybGgwkdbMsgVRVh4BnJHkc8N4kT6+qm1eeT7IT2AlwCqd2qlKbwsi3fbvLTmqkqr4KfBTYserxXVW1vaq2n8jJfYqTRsBAUjcry3YbvfWU5AnTyYgk3wG8APhc16K0OVXDWycu2amb3mHSyJOAt0+PIz0GeHdVvb9zTdqsRt5SBpK6WYZAqqqbgGf2rkOC8W9qcMlOkjQITkjqYgjHf6SlM/KWMpDUjYEkNTbylnLJTpI0CE5I6sYJSWonNf5NDQaSujGQpMb8pgZJkjbOCUndOCFJjY28pQwkdeG2b6m9sR9DcslOkjQITkjqxglJamzkLWUgqRsDSWpoCbZ9u2QnbVCSLUk+k8Rv+ZY2wAlJ3SzRhPRq4DbgjN6FaJMbeUs5IambJblA31bgx4A3dy1EgtFfoM9AkjbmDcAvAYeP9oIkO5PsTbL3IR5YXGXSyLhkpy6GMN1sVJIfB+6qqhuSPPdor6uqXcAugDPyneP+0Bq0sW9qMJDUzdgDCXgO8KIkFwGnAGck+f2q+unOdUmj5JKdtE5V9dqq2lpV5wKXAB8xjKT1c0JSN0swIUnDMvKWMpDUzTIFUlVdC1zbuQxtZp4YK0lSG05I6maZJiRpEEbeUgaSuliGbd/S4Iy8pVyykyQNwugnpGTc15DfzJyQhulHz35G7xK0DmH8mxpGH0gaLwNJamzkLeWSnSRpEJyQ1I0TktTQEpyHZCCpGwNJamzkLeWSnTaFJDuS3J5kX5LLH+V1L05SSbYvsj5pbObRU05I6mKR5yEl2QJcCbwAOABcn2R3Vd266nWnM7n663ULKUxqbUET0rx6yglJ3SzwirEXAvuqan9VPQhcBVx8hNf9W+D1wP3H8zmSfD7JZ5PcmGTv8fxZqaVUm9sazKWnDCQtg7NWrsg6ve1c9fw5wB0z9w9MH/uWJM8CtlXV/1xnDc+rqmdUlUt9WgZdesolO3XTcMnu4EaCIMljgN8CXt6qIKmLdkt2XXrKCUndLHDJ7k5g28z9rdPHVpwOPB24Nsnnge8Ddh/HxoYC/jjJDUf4TZIkO1d+03yIB9b4ltJxqoa3Y5tLTzkhaTO4Hjg/yXlMmuYS4KUrT1bV14CzVu4nuRb4xapa6/GgH6iqO5N8F/DBJJ+rqo/NvP8uYBfAGfnOkW/MlYA59ZQTkrpZ1IRUVQ8DlwHXALcB766qW5JckeRFDT7HndP/3gW8l8kBX2nhFrWpYV495YSkLhZ9+Ymq2gPsWfXY647y2ueu9X2TnAY8pqrum/78D4ArNlCqtH4LnL/n0VMGkrQxTwTeO/3W+ROAP6iqD/QtSRonA0ndLMNXB1XVfuB7e9chgd9lJ63bMgSSNCgjbyk3NUiSBsEJSd04IUkNrf0cosEykNSNgSS1k+ltzFyykyQNghOSulj0eUjSpjDyljKQ1I2BJLU19m3fLtlJkgbBCUndOCFJjY28pQwkdWMgSY2NvKVcspMkDYITkrpZlgkpyeOANzO5IFkBP1dVn+xblTadNV46YsgMJHWxZNu+3wh8oKpekuQk4NTeBWmTGnlLGUjqZhkCKcmZwA8CLweoqgeBB3vWpM1r7BOSx5CkjTkPuBv43SSfSfLm6YX6viXJziR7k+x9iAf6VCmNgIGkbhZ1CfM5OwF4FvCmqnom8E3g8tkXVNWuqtpeVdtP5OQeNWqzqEa3TlyyUzcDCJMWDgAHquq66f2rWRVI0qK4ZCdtYlX1ZeCOJE+bPvR84NaOJUmj5YSkbpZkQgJ4FfDO6Q67/cArOtejzcjrIUnrM5DjP01U1Y3A9t51SGMPJJfsJEmD4ISkbpZlQpKGIIx/U4OBpG4MJKmxkbeUS3aSpEFwQlI3TkhSWxl5Tz1qIFVVFlWINpdl2mV3PO7j3oMfqqu/sIaXngUcnHc9G2SNG7fW+p58zFe47VvS8aiqJ6zldUn2VtWgt5Jb48YNvb5FM5DUzWackKR5cpedtE4GktTYyFvKXXbSMO3qXcAaWOPGDb2+hXJCUjdOSEdXVYP/h8oaN651fS7ZSetkIEmNjbylXLKTBibJjiS3J9mXZHDXVkqyLclHk9ya5JYkr+5d05Ek2TK9iu/7e9dyJEkel+TqJJ9LcluS7+9dU29OSOpis56HdCxJtgBXAi9gcvG/65PsrqohXWPpYeA1VfXpJKcDNyT54MBqBHg1cBtwRu9CjuKNwAeq6iXTS5ecuqF3q/Ev2TkhqZsluYR5axcC+6pqf1U9CFwFXNy5pm9TVV+qqk9Pf76PyT/65/St6tsl2Qr8GPDm3rUcSZIzgR8E3gJQVQ9W1Vc3/MYjv4S5gSQNyznAHTP3DzCwf+xnJTkXeCZw3aO/cuHeAPwScLh3IUdxHnA38LvTZcU3Jzmtd1G9GUjqxglp3JI8FvhD4Oer6uu961mR5MeBu6rqht61PIoTgGcBb6qqZwLfBDZ0vHDl8hMtbr14DEndGCZHdCewbeb+1uljg5LkRCZh9M6qek/velZ5DvCiJBcBpwBnJPn9qvrpznXNOgAcqKqVyfJqNhhIAIy8p5yQpGG5Hjg/yXnTA92XALs71/RtkoTJsY/bquq3etezWlW9tqq2VtW5TP7/fWRgYURVfRm4I8nTpg89HxjappCFc0JSN05Ij1RVDye5DLgG2AK8tapu6VzWas8Bfgb4bJIbp4/9clXt6VjTGL0KeOf0F4/9wCs2+oZj32VnIKkLj/8c3fQf9sH+415VH2dyyGLwqupa4NrOZRxRVd0ItPum7yW4/IRLdpKkQXBCUjdOSFJbGeom9zUykNSNgSQ1NvKWcslOkjQITkjqxglJastddtI6GUhSQ4UnxkqS1IITkrrwPCSpPZfspHUykKTGRt5SLtlJkgbBCUndOCFJ7axcfmLMDCR1YyBJDVW5y06SpBackNSNE5LUlkt20jq47Vuag5G3lEt2kqRBcEJSN05IUlsu2UnrZCBJDRVweNw95ZKdJGkQnJDUjROS1NjIW8pAUjcGktTW2I8huWQnSRoEJyR14XlI0hyMvKcMJHVjIEltuWQnjUCSHUluT7IvyeVHeP4Xktya5KYkH07y5B51SmMxj54ykNTNyrLdRm/HkmQLcCXwQuAC4NIkF6x62WeA7VX1d4Grgf/Y+ONK81UNb8cwr54ykNTNogIJuBDYV1X7q+pB4Crg4lW1fLSq/nJ691PA1qYfVpqzyfWQqsltDebSUx5DUjcNjyGdlWTvzP1dVbVr5v45wB0z9w8Az36U93sl8EetipMW5nCzd+rSUwaSlsHBqtre4o2S/DSwHfihFu8njVSXnjKQ1MWCt33fCWybub91+ti3SfIjwK8AP1RVDyyoNqmZNS63tTCXnjKQ1M0CA+l64Pwk5zFpmkuAl86+IMkzgf8K7KiquxZVmNTMGjckNDKXnnJTg5ZeVT0MXAZcA9wGvLuqbklyRZIXTV/2n4DHAv89yY1JdncqVxq8efWUE5K6WeSJsVW1B9iz6rHXzfz8IwsrRpqLWug3NcyjpwwkdeM3NUht+U0NkiQ14ISkbpyQpMZG3lMGkrrw276lxgrS7sTYLlyykyQNghOSunFCkhobeU8ZSOrGQJIaG3lLuWQnSRoEJyR144QktbXA77KbCwNJ3RhIUmMj7ymX7CRJg+CEpC48D0lqrGh5gb4uDCR1YyBJ7YQ1X358sFyykyQNghOSunFCkhobeU8ZSOrGQJIaG3lPuWQnSRoEJyR144QkNeQuO2l93PYttecuO0mSGnBCUjdOSFJjI+8pA0ndGEhSSzX6QHLJTpI0CE5I6sYJSWqoGP2EZCCpGwNJamzk275dspMkDYITkrrwPCSpvbGfh2QgqRsDSWps5D3lkp0kaRCckNSNE5LUUAGHx91TBpK6MZCkljwxVpKkJpyQ1I0TktTYyHvKQFIXbvuW5mDkPeWSnSRpEJyQ1I0TktSQu+yk9TOQpJYKatxfZueSnSRpEJyQ1I0TktTYyHvKQFI3BpLU0BIcQ3LJTpI0CE5I6sLzkKQ5GHlPGUjqxkCSGht5T7lkJ0kaBCckdeOEJLU0/m/7NpDUjYEkNVTAYU+MlSRpw5yQ1I0TktTYyHvKQFIXbvuW5mDkPWUgqRsDSWqp/KYGSZJacEJSN05IUkMFNfLLTxhI6sZAkhpzyU6SpI1zQlI3TkhSYyPvKQNJXbjtW2qsym9qkCSpBSckdeOEJDU28p4ykNSNgSS1VS7ZSZK0cU5I6sYJSWrJ6yFJ62YgSQ0VnhgrSVILTkjqwvOQpDnwu+yk9TGQpHYKKJfsJEnaOCckdeOEJDVUNfolOyckdbNyHGmjt7VIsiPJ7Un2Jbn8CM+fnOS/TZ+/Lsm5jT+uNHd1uJrc1mIePWUgaekl2QJcCbwQuAC4NMkFq172SuDeqnoK8J+B1y+2Smk85tVTBpK6WeCEdCGwr6r2V9WDwFXAxateczHw9unPVwPPT5JmH1ZahDrc5nZsc+kpjyGpl2uAsxq91ylJ9s7c31VVu2bunwPcMXP/APDsVe/xrddU1cNJvgY8HjjYqEZpru7j3ms+VFePuqcMJHVRVTt61yAtk2XoKZfstBncCWybub91+tgRX5PkBOBM4CsLqU4an7n0lIGkzeB64Pwk5yU5CbgE2L3qNbuBn53+/BLgI+W+dOlo5tJTLtlp6U3Xry9jctxqC/DWqrolyRXA3qraDbwFeEeSfcA9TBpM0hHMq6fiL4GSpCFwyU6SNAgGkiRpEAwkSdIgGEiSpEEwkCRJg2AgSZIGwUCSJA3C/wfeT3NIHCn5dwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating a brand new folder"
      ],
      "metadata": {
        "id": "h0If2-lO6C_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a folder if doesn't exist\n",
        "\n",
        "def createDir(path):\n",
        "  doesExist = os.path.exists(path) # checks whether the specified path exists\n",
        "  if not doesExist:\n",
        "    os.makedirs(path) # create path, since it doesn't exist\n",
        "    print(\"The new directory was created!\")\n",
        "\n",
        "# create images folder in our folder_path\n",
        "images_path = f'{folder_path}/images' # your folder you want to create!\n",
        "createDir(images_path)"
      ],
      "metadata": {
        "id": "Q6-8utPD6G0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Copying a folder and its subdirectories"
      ],
      "metadata": {
        "id": "y1IDZlyd8T5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = '/some/path'\n",
        "dst = '/other/one'\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "id": "JwN2fAA_8hzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Histograms"
      ],
      "metadata": {
        "id": "feVGhy1Y9Fv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the histogram of the red channel of an image\n",
        "img_copy = img.copy()\n",
        "hist = cv2.calcHist([img_copy],[0],None,[256],[0,256])\n",
        "# our image, first channel(0=red), no mask, 256 bins, range 0-255\n",
        "  \n",
        "# plot the above computed histogram\n",
        "plt.plot(hist, color='r') # red color for the line\n",
        "plt.title('Image histogram for the red channel of an image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "##################################\n",
        "\n",
        "# Plotting multiple channels:\n",
        "# computing the histogram of all channels of an image\n",
        "img_copy = img.copy()\n",
        "  \n",
        "# plot the above computed histogram\n",
        "colors = ('r','g','b') # for the line color\n",
        "\n",
        "for i,color in enumerate(colors):\n",
        "  hist = cv2.calcHist([img],[i],None,[256],[0,256]) # our image, channel(0/1/2), no mask, 256 bins, range 0-255\n",
        "  plt.plot(hist, color = color) # r/g/b color for each line\n",
        "plt.title('Image histogram (RGB) of an image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "#########################################################################\n",
        "\n",
        "# using plt to plot a histogram (one channel)\n",
        "\n",
        "hist_with_plt = plt.hist(img[:,:,1].flatten(), bins = 256, color='g') # Selecting the green channel\n",
        "plt.title('Image histogram (green layer) of an image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.legend(['Total', 'Red Channel', 'Green Channel', 'Blue Channel']) # adding a legend\n",
        "plt.show()\n",
        "\n",
        "#################\n",
        "\n",
        "# using plt to plot a histogram (multi channel)\n",
        "\n",
        "colors = ('r','g','b') # for the line color\n",
        "\n",
        "# plotting the total histogram, will show up in the back\n",
        "hist = plt.hist(img.flatten(), bins = 256, color = 'orange') \n",
        "\n",
        "#loop to plot for each channel\n",
        "for i,color in enumerate(colors):\n",
        "  hist = plt.hist(img[:, :, i].flatten(), bins = 256, color = color, alpha = 0.5)\n",
        "\n",
        "# some more details for the figure\n",
        "plt.title('Image histogram (RGB) of an image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.legend(['Total', 'Red Channel', 'Green Channel', 'Blue Channel']) # adding a legend\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RtYVvmO89KoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting an image to grayscale"
      ],
      "metadata": {
        "id": "c-WHT2yP97cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First method:\n",
        "img = io.imread(imgPath, as_gray=True)\n",
        "\n",
        "# Second method:\n",
        "img = io.imread(imgPath)\n",
        "gray_image = skimage.color.rgb2gray(img)"
      ],
      "metadata": {
        "id": "q8E_sP-h9_w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operations on images"
      ],
      "metadata": {
        "id": "kk1pSQW-B41s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Addition"
      ],
      "metadata": {
        "id": "QBOfHrIOGxhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First of all, the two images needs to be in the same dimensions and normalized\n",
        "\n",
        "img1 = io.imread(path1)\n",
        "img2 = io.imread(path2)\n",
        "\n",
        "resized1 = cv2.resize(img1, (img2_width,img2_height), interpolation = cv2.INTER_AREA)\n",
        "\n",
        "addition = img_as_float(resized1) + img_as_float(img2)"
      ],
      "metadata": {
        "id": "w6k0-i74CJWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multlipication"
      ],
      "metadata": {
        "id": "LZEqP-iGHHZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multlipication by a scalar (n > 1) lightens the image\n",
        "\n",
        "dark_img = io.imread(pathKolsheu)\n",
        "dark_img = img_as_float(dark_img)\n",
        "\n",
        "light_img = dark_img * 3"
      ],
      "metadata": {
        "id": "24XmYV8PHJHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Division\n",
        "It is used for dealing with uneven lighting (For example, deleting shadows)."
      ],
      "metadata": {
        "id": "6uI1Cq6hIALx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "objects_uneven_lighting = io.imread(o_u_l_path)\n",
        "uneven_lighting = io.imread(u_l_path)\n",
        "\n",
        "quotient = objects_uneven_lighting / uneven_lighting # This operation gives you the objects with little to no shadow"
      ],
      "metadata": {
        "id": "unDz5FayID-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you subtract the uneven lighting from the picture with the uneven lit objects, you get\n",
        "ASK YEDIDIAH ABOUT THIS\n",
        "Is it correct to say that we get the object as grayscale without the background?"
      ],
      "metadata": {
        "id": "rl8kmD5_Jc0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image filtering"
      ],
      "metadata": {
        "id": "LeCdUUOXLUbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mean filter"
      ],
      "metadata": {
        "id": "S7eMsDBpLYyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining our mean kernel (3,3) with values of 1/9\n",
        "mean_kernel = np.full((3, 3), 1/9)\n",
        "\n",
        "%precision 2\n",
        "print('Our original array:\\n',bright_square)\n",
        "\n",
        "new = ndi.correlate(bright_square, mean_kernel)\n",
        "print('\\nOur convolved array:\\n',new)\n",
        "\n",
        "plt.imshow(new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "TuAEflOZLdQi",
        "outputId": "d4e3ff12-b96d-495e-c5af-c6c95ab8186a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our original array:\n",
            " [[0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 1. 1. 0. 0.]\n",
            " [0. 0. 1. 1. 1. 0. 0.]\n",
            " [0. 0. 1. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "Our convolved array:\n",
            " [[0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.11 0.22 0.33 0.22 0.11 0.  ]\n",
            " [0.   0.22 0.44 0.67 0.44 0.22 0.  ]\n",
            " [0.   0.33 0.67 1.   0.67 0.33 0.  ]\n",
            " [0.   0.22 0.44 0.67 0.44 0.22 0.  ]\n",
            " [0.   0.11 0.22 0.33 0.22 0.11 0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.  ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f43060cea50>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pixelating an image\n",
        "Sampling every n-th pixel"
      ],
      "metadata": {
        "id": "VljIiMtjMH2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = io.imread(somePath)\n",
        "pixelated = img[0::10, 0::10] # For example - sampling every 10th pixel."
      ],
      "metadata": {
        "id": "8HYVjewDMeAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gaussian blur\n",
        "* Blurring is used as a first step before we perform thresholding or edge detection.\n",
        "* Larger sigma will reduce more noise, though it might remove crucial details from the image."
      ],
      "metadata": {
        "id": "_dFgbRaoNmaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Via skimage:\n",
        "sigma = 1 # For the gaussian filter (Standard deviation for the gaussian curve)\n",
        "smooth = filters.gaussian(bright_square, sigma)\n",
        "plt.imshow(smooth)\n",
        "\n",
        "# Via cv2:\n",
        "gaussian_using_cv2 = cv2.GaussianBlur(bright_square, (3,3), 0, borderType=cv2.BORDER_CONSTANT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "DxvUUi9KNoHc",
        "outputId": "a075c4dc-0ac4-4552-ac1d-815aa56ef04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f42f6016650>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QaHmRdmeO6Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Unsharp image\n",
        "A method for edge enhancing.<br>\n",
        "We take an unsharp version of an image, and subtract it from the original."
      ],
      "metadata": {
        "id": "PSDdtrSPP3KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = img_as_float(io.imread(image_path, as_gray=True)) # Original\n",
        "gaussian_img = gaussian(img, sigma=2, mode='constant', cval=0.0) # Creating our blurred version\n",
        "enhanced_image = img + (img - gaussian_img)*1 # Enhanced image = original + amount * (original - blurred)"
      ],
      "metadata": {
        "id": "kT_liFzgP5Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Basic edge filtering"
      ],
      "metadata": {
        "id": "TQNeQm8e7-04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math # IMPORTANT! Otherwise you wouldn't be able to use the square root function.\n",
        "\n",
        "# Vertical/horizontal kernels\n",
        "vertical_kernel = np.array([\n",
        "    [-1],\n",
        "    [ 0],\n",
        "    [ 1],\n",
        "])\n",
        "\n",
        "horizontal_kernel = np.array([\n",
        "    [-1, 0, 1]\n",
        "])\n",
        "\n",
        "# Convolutions\n",
        "gradient_vertical = ndi.correlate(img.astype(float),\n",
        "                                  vertical_kernel)\n",
        "gradient_horizontal = ndi.correlate(img.astype(float),\n",
        "                                  horizontal_kernel)\n",
        "\n",
        "# Calculating the magnitude\n",
        "magnitude = np.zeros((img.shape))\n",
        "for i in range(img.shape[0]):\n",
        "  for j in range(img.shape[1]):\n",
        "    magnitude[i][j] = math.sqrt(gradient_horizontal[i][j]**2 + gradient_vertical[i][j]**2)"
      ],
      "metadata": {
        "id": "bq-1NUk08EHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sobel edge detection\n",
        "The most commonly used edge filter."
      ],
      "metadata": {
        "id": "dDDtIP4O9f5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smooth = filters.gaussian(img_as_float(img), sigma=1) # Smoothing is often used as a preprocessing for edge detection\n",
        "img_edges = filters.sobel(smooth) # should look pretty similar to the basic edge detection method"
      ],
      "metadata": {
        "id": "nyLcSwa09jgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Canny edge detection"
      ],
      "metadata": {
        "id": "ZlE36MWG_iwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The Process of Canny edge detection algorithm can be broken down to 5 different steps: (ALL OF THIS HAAPPENS UNDER THE HOOD)\n",
        "\n",
        "\n",
        "1.   Apply Gaussian filter to smooth the image in order to remove the noise\n",
        "2.   Find the intensity gradients of the image\n",
        "3. Apply non-maximum suppression to get rid of spurious response to edge detection\n",
        "4. Apply double threshold to determine potential edges (supplied by the user)\n",
        "5. Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges."
      ],
      "metadata": {
        "id": "5YQSudCdAf0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Canny\n",
        "canny_edge = cv2.Canny(img, 195, 200)  #Supply Thresholds 1 and 2 "
      ],
      "metadata": {
        "id": "_DYPqyWvBVJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Denoising filters"
      ],
      "metadata": {
        "id": "oNXyuUymDyqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Median filter"
      ],
      "metadata": {
        "id": "m9PzMu4vEnSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"structuring element\", matrix of ones though corners are zeroes\n",
        "neighborhood = disk(radius=1)  \n",
        "\n",
        "# applying the median filter, needs 8 bit, not float.\n",
        "median = filters.rank.median(img, neighborhood) "
      ],
      "metadata": {
        "id": "_HsWN90zD-GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Bilateral filter\n",
        "It's not an efficient filter, but it's also an option."
      ],
      "metadata": {
        "id": "2dVBIKIgFUy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = img_as_ubyte(original_img)\n",
        "bilateral_using_cv2 = cv2.bilateralFilter(img, 5, 20, 100, borderType=cv2.BORDER_CONSTANT)\n"
      ],
      "metadata": {
        "id": "wBuCReExFX8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####NLM- Non local means\n",
        "Works well for random gaussian noise, but not for salt and pepper."
      ],
      "metadata": {
        "id": "Y_r57Z1MHeTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigma_est = np.mean(estimate_sigma(img, multichannel=True)) # Providing an automated guess of the sigma value\n",
        "\n",
        "denoised_img= denoise_nl_means(img, h=5 * sigma_est, fast_mode=True,\n",
        "                               patch_size=5, patch_distance=3, multichannel=False)"
      ],
      "metadata": {
        "id": "eGPYGABiH9qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####TVF- Total variation filter"
      ],
      "metadata": {
        "id": "P5zl5Iq3I4gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First you will need to convert the image to be represented with floats\n",
        "denoised_img = denoise_tv_chambolle(img, weight=0.1, eps=0.0002, n_iter_max=200, multichannel=False)"
      ],
      "metadata": {
        "id": "g2hu31vzI9E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fourier transformation\n",
        "Fourier transformation converts an image (or any signal) from spatial (or time) domain to frequency domain. Here's an example with Python."
      ],
      "metadata": {
        "id": "StJ1Fb3LR43s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying discrete fourier transform \n",
        "# notice that cv2.dft takes only float32\n",
        "dft = cv2.dft(np.float32(img), flags=cv2.DFT_COMPLEX_OUTPUT) \n",
        "\n",
        "#Shift DFT.\n",
        "# (if we don't shift then the ft will be at the top of the image,\n",
        "# and won't be seen)\n",
        "dft_shift = np.fft.fftshift(dft) \n",
        "\n",
        "#Calculate magnitude spectrum from the DFT\n",
        "#Added 1 as we may see 0 values and log of 0 is indeterminate\n",
        "magnitude_spectrum = 20 * np.log((cv2.magnitude(dft_shift[:, :, 0], dft_shift[:, :, 1]))+1)\n",
        "\n",
        "#As the spatial frequency increases (bars closer), \n",
        "#the peaks in the DFT amplitude spectrum move farther away from the origin\n",
        "\n",
        "plt.imshow(magnitude_spectrum)\n",
        "plt.title.set_text('FFT of image')"
      ],
      "metadata": {
        "id": "UqHIPwcxSEYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Segmentation"
      ],
      "metadata": {
        "id": "mw0i9-NyTboH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Thresholding\n",
        "Thresholding is a type of image segmenntation, where we change the pixels of an image to make it easier to annalyze. "
      ],
      "metadata": {
        "id": "biIvjGFUThtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Using histograms"
      ],
      "metadata": {
        "id": "bt7zh1DxUn-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Manually\n",
        "\n",
        "\n",
        "*   Read an image\n",
        "*   Plot its histogram\n",
        "*   Choose a Threshold that matches your ROI.\n",
        "*   Create a boolean mask with the threshold value.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ii3arUE8UutL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Mannually using OpenCV"
      ],
      "metadata": {
        "id": "PelCTvK0V_NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using opencv to perform manual threshold\n",
        "# All pixels above 0.55 will have pixel value 1\n",
        "# Should be exactly same as the above method. \n",
        "ret1, thresh1 = cv2.threshold(img, 0.55, 1, cv2.THRESH_BINARY)\n",
        "plt.imshow(thresh1, cmap='gray')\n",
        "plt.title('Our mask, using cv2')"
      ],
      "metadata": {
        "id": "MROZlRAwWEsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Auto thresholding using otsu"
      ],
      "metadata": {
        "id": "ooExDqU8WpYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using cv2 for otsu based automatic thresholding\n",
        "ret2, thresh2 = cv2.threshold(img_as_ubyte(img),0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "print('Threshold value by otsu is: ', ret2)"
      ],
      "metadata": {
        "id": "D1mmrVIyWvWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Multi thresholding"
      ],
      "metadata": {
        "id": "wfaww9CcYTEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, you need to show histogram to see which pixel range\n",
        "# is good for the ROI.\n",
        "hist_with_plt = plt.hist(img.flatten(), bins = 256, color='gray')\n",
        "\n",
        "# Apply multi-Otsu threshold \n",
        "thresholds = threshold_multiotsu(img, classes=4)\n",
        "# We chose classes=4, which results with three values-\n",
        "# thus, segmenting the image to 4 different regions.\n",
        "\n",
        "# Digitize (segment) original image into multiple classes.\n",
        "# np.digitize assign values 0, 1, 2, 3, ... to pixels in each class.\n",
        "regions = np.digitize(img, bins=thresholds)\n",
        "plt.imshow(regions)\n",
        "\n",
        "# Looking at a specific region\n",
        "plt.imshow(regions==3, cmap='gray')"
      ],
      "metadata": {
        "id": "-OreWELfYYpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Entropy\n",
        "This functioin gives a value that represent the amount of complexity in a certain section of an image, in a relation to a certain structuring element that we choose. "
      ],
      "metadata": {
        "id": "-MM9EdqfaMrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the disk size to get better results\n",
        "entropy_img = entropy(img, disk(7)) \n",
        "plt.imshow(entropy_img)\n",
        "\n",
        "# Thresholding using otsu\n",
        "plt.hist(entropy_img.flat, bins=100, range=(0,5))\n",
        "thresh = threshold_otsu(entropy_img)\n",
        "binary = entropy_img <= thresh\n",
        "\n",
        "# Filling in the holes (if needed)\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(8,8)) # define a kernel (change size if needed)\n",
        "res = cv2.morphologyEx(img_as_ubyte(binary),cv2.MORPH_OPEN,kernel) # applying the kernel to our binary (make su\n",
        "\n",
        "# inverting the values, if needed\n",
        "new_mask = cv2.bitwise_not(res)\n",
        "\n",
        "# display the results\n",
        "# if you didn't invert the image, use:\n",
        "# plt.imshow(res*left_img, cmap='gray')\n",
        "plt.imshow(new_mask*left_img, cmap='gray')"
      ],
      "metadata": {
        "id": "i0SbkakHan49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing python file as module"
      ],
      "metadata": {
        "id": "5h1G1-gc0RbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the working directory\n",
        "!pwd # ! interacts with the operating system"
      ],
      "metadata": {
        "id": "KjTRHZL_0VhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing it\n",
        "%cd /content/drive"
      ],
      "metadata": {
        "id": "Mi8N7TYH0ki2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing it to where our python scripts are, for example:\n",
        "%cd /content/drive/MyDrive/71254_2023/01_Lectures/Class06/scripts"
      ],
      "metadata": {
        "id": "PehX9FX302Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying to import the file 'display_img'\n",
        "import display_image"
      ],
      "metadata": {
        "id": "nC4WgVrd08pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_of_image = '/content/drive/MyDrive/71254_2023/01_Lectures/Class06/images/cucs.jpg' # from the drive\n",
        "# using a function I wrote within the library\n",
        "display_image.display_img_from_path(path_of_image) "
      ],
      "metadata": {
        "id": "8vj5B0o61XuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmentation continuation"
      ],
      "metadata": {
        "id": "Vl8RsF2x1lNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Watershed"
      ],
      "metadata": {
        "id": "jxb5BmQH15ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-processing\n",
        "img = cv2.imread(f'{images_path}/watershed_coins.jpg') # read image\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # bgr to rgb\n",
        "\n",
        "# mean shift filtering to aid the thresholding step. only for rgb.\n",
        "shifted = cv2.pyrMeanShiftFiltering(img, 21, 51)\n",
        "gray = cv2.cvtColor(shifted, cv2.COLOR_BGR2GRAY) # color to gray\n",
        "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "# compute the exact Euclidean distance from every binary\n",
        "# pixel to the nearest zero pixel, then find peaks in this\n",
        "# distance map\n",
        "D = ndimage.distance_transform_edt(thresh)\n",
        "print(D)\n",
        "# Now we take D , our distance map, and find peaks (i.e., local maxima) in the map. Well ensure that is at least a 20 pixel distance between each peak.\n",
        "localMax = peak_local_max(D, indices=False, min_distance=10,labels=thresh)\n",
        "\n",
        "# perform a connected component analysis on the local peaks,\n",
        "# using 8-connectivity, then appy the Watershed algorithm\n",
        "markers = ndimage.label(localMax, structure=np.ones((3, 3)))[0]\n",
        "labels = watershed(-D, markers, mask=thresh) #  matrix of labels\n",
        "print(\"[INFO] {} unique segments found\".format(len(np.unique(labels)) - 1))\n",
        "\n",
        "# loop over the unique labels returned by the Watershed algorithm\n",
        "total_area = [] # list to store areas of onjects\n",
        "\n",
        "for i,label in enumerate(np.unique(labels)):\n",
        "\t# if the label is zero, we are examining the 'background'\n",
        "\t# so simply ignore it\n",
        "\tif label == 0:\n",
        "\t\tcontinue\n",
        "\t# otherwise, allocate memory for the label region, and set the pixels belonging to the current label to 255 (white). draw it on the mask.\n",
        "\tmask = np.zeros(gray.shape, dtype=\"uint8\")\n",
        "\tmask[labels == label] = 255\n",
        "\n",
        "\n",
        "\t# detect contours in the mask and grab the largest one  this contour will represent the outline/boundary of a given object in the image.\n",
        "\tcnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
        "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
        "\tcnts = imutils.grab_contours(cnts)\n",
        "\tc = max(cnts, key=cv2.contourArea)\n",
        "\t\n",
        "\t# calc area and append to list\n",
        "\tarea = cv2.contourArea(c)\n",
        "\ttotal_area.append(area)\n",
        "\tprint(f'Object number {i} has an area = ', area)\n",
        " \t\n",
        " \n",
        "\t# draw the contours enclosing the object\n",
        "\t((x, y), r) = cv2.minEnclosingCircle(c)  # We find the circumcircle of an object using the function cv.minEnclosingCircle(). It is a circle which completely covers the object with minimum area.\n",
        "\tcv2.putText(img_rgb, \"#{}\".format(label), (int(x) - 15 , int(y)), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 0.7, (0, 0, 255), 2) # draw the ID on the object\n",
        "\tcv2.drawContours(img_rgb, cnts, -1, (0,255,0), 1) # draw the counters\n",
        " \n",
        "# show the output image\n",
        "plt.imshow(img_rgb)"
      ],
      "metadata": {
        "id": "uOUsIMqg1qRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Voronoi"
      ],
      "metadata": {
        "id": "4wb48H3G3-ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing pyclesperanto, a library that enhances our gpu\n",
        "!pip install pyclesperanto-prototype"
      ],
      "metadata": {
        "id": "G9C6mQDD5Bu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# otsu + vor + labeling\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pyclesperanto_prototype as cle\n",
        "from skimage import exposure, img_as_ubyte\n",
        "\n",
        "#Read the input image\n",
        "# read an image of cells\n",
        "img = io.imread(f'{images_path}/cells.jpg', as_gray = True)\n",
        "\n",
        "# display it\n",
        "plt.imshow(img, cmap='gray')\n",
        "\n",
        "#Normalize then scale to 255 and convert to uint8 - using skimage\n",
        "cells_8bit = img_as_ubyte(img)\n",
        "plt.imshow(cells_8bit, cmap='gray')"
      ],
      "metadata": {
        "id": "BYrI5-K-5M6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing our GPU\n",
        "\n",
        "# list names of all available GPU-devices\n",
        "print(\"Available devices:\" + str(cle.available_device_names()))"
      ],
      "metadata": {
        "id": "7ILBtAcK5_in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a specific GPU device from the above\n",
        "device = cle.select_device('Tesla T4')\n",
        "print(\"Used GPU: \", device)\n",
        "\n",
        "#Push the image to gpu memory\n",
        "cells_gpu = cle.push(cells_8bit)\n",
        "print(\"Image size in GPU: \" + str(cells_gpu.shape))\n",
        "\n",
        "# display\n",
        "cle.imshow(cells_8bit, color_map='gray')"
      ],
      "metadata": {
        "id": "b-zEF_y36Ag3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ voronoi_otsu_labeling library ##################\n",
        "# voronoi_otsu_labeling(image, spot_sigma=some_number, outline_sigma=another_number)\n",
        "#spot_sigma= depends on how close the detected objects can be. Low number may divide large objects into multiple objects.\n",
        "#outline_sigma = how precise the outline needs to be for the segmented objects (use a low number)\n",
        "segmented = cle.voronoi_otsu_labeling(cells_gpu, spot_sigma=5, outline_sigma=1)\n",
        "cle.imshow(segmented, labels=True)"
      ],
      "metadata": {
        "id": "f_hoU8806I9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###K-Means"
      ],
      "metadata": {
        "id": "-_82Lq8A7QpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-means segmentation\n",
        "# applied on RGB image, if you want to apply it to a grayscale image, \n",
        "# make sure that you update the proper parameters.\n",
        "\n",
        "# reshape the image to a 2D array of pixels and 3 color values (RGB)\n",
        "pixel_values = output.reshape((-1, 3))\n",
        "# convert to float\n",
        "pixel_values = np.float32(pixel_values)\n",
        "\n",
        "# define stopping criteria\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
        "\n",
        "# number of clusters (K)\n",
        "k = 6\n",
        "_, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "# convert back to 8 bit values\n",
        "centers = np.uint8(centers)\n",
        "\n",
        "# flatten the labels array\n",
        "labels = labels.flatten()\n",
        "\n",
        "# convert all pixels to the color of the centroids\n",
        "segmented_image = centers[labels.flatten()]\n",
        "\n",
        "# reshape back to the original image dimension\n",
        "segmented_image = segmented_image.reshape(output.shape)\n",
        "\n",
        "# show the image\n",
        "plt.title(\"K-Means Segmentation\")\n",
        "plt.axis('off')\n",
        "plt.imshow(segmented_image,cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Js8Z5u917TT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a look at  the ceneters of each cluster\n",
        "print(centers)"
      ],
      "metadata": {
        "id": "hbn7rGJC8FeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Colab forms\n"
      ],
      "metadata": {
        "id": "MkuE9Yjs8mSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title String fields\n",
        "\n",
        "text = 'How are you doing?' #@param {type:\"string\"}\n",
        "dropdown = \"Great!\" #@param [\"Great!\", \"Ok\", \"Could be better\"]\n",
        "text_and_dropdown = '1st option' #@param [\"1st option\", \"2nd option\", \"3rd option\"] {allow-input: true}\n",
        "\n",
        "print(text)\n",
        "print(dropdown)\n",
        "print(text_and_dropdown)"
      ],
      "metadata": {
        "id": "Sg76p_758pJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Date fields\n",
        "date_input = '1997-07-14' #@param {type:\"date\"}\n",
        "\n",
        "print(date_input)"
      ],
      "metadata": {
        "id": "PUgpozdZ8wXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number fields\n",
        "number_input = 19 #@param {type:\"number\"}\n",
        "number_slider = -2.2 #@param {type:\"slider\", min:-10, max:10, step:0.1}\n",
        "\n",
        "integer_input = 20 #@param {type:\"integer\"}\n",
        "integer_slider = -8 #@param {type:\"slider\", min:-8, max:100, step:1}\n",
        "\n",
        "print(number_input)\n",
        "print(number_slider)\n",
        "\n",
        "print(integer_input)\n",
        "print(integer_slider)"
      ],
      "metadata": {
        "id": "Rt7aPy6l8yD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title UPLOAD IMAGES HERE: RUN ME.  { display-mode: \"form\" }\n",
        "import google\n",
        "\n",
        "try:\n",
        "  files = google.colab.files\n",
        "  uploaded = google.colab.files.upload()\n",
        "except:\n",
        "  print(\"\")\n",
        "  print(\"Please use Chrome, and enable cookies!\")\n",
        "  print(\"cookie      ,    \")\n",
        "\n",
        "# lets the user upload the file, and stores the file name in 'file_names'. The files are uploaded under /content/FILE_NAME\n",
        "file_names = uploaded.keys() "
      ],
      "metadata": {
        "id": "TZjnmpct85Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets convert our file names into a list of of file names\n",
        "list_file_paths = list(file_names)\n",
        "list_file_paths\n",
        "plt.imshow(io.imread(list_file_paths[0]))\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "8szJ9v5o85Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloading files: Example, downloading the first uploaded file above. RUN ME!  { display-mode: \"form\" }\n",
        "files.download(list_file_paths[0])"
      ],
      "metadata": {
        "id": "ozh3P_Fm9JIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AruCo marker + MGVRI calculation"
      ],
      "metadata": {
        "id": "7uKSACtO9P0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read an image\n",
        "image =io.imread(f'{folder_path}/test.jpg')\n",
        "\n",
        "# making a copy\n",
        "copy = image.copy()\n",
        "# Load Aruco detector\n",
        "parameters = cv2.aruco.DetectorParameters_create()\n",
        "aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_5X5_50)\n",
        "# Get Aruco marker\n",
        "corners, _, _ = cv2.aruco.detectMarkers(copy, aruco_dict, parameters=parameters)\n",
        "# Draw polygon around the marker\n",
        "int_corners = np.int0(corners)\n",
        "cv2.polylines(copy, int_corners, True, (0, 255, 0), 10)\n",
        "\n",
        "# Aruco Area\n",
        "aruco_area = cv2.contourArea (corners[0])\n",
        "print('AruCo Area:',aruco_area, 'px')\n",
        "\n",
        "# Pixel to cm ratio\n",
        "pixel_cm_ratio = 10*10 / aruco_area# since the AruCo is 10*10 cm, so we devide 100 cm*cm by the number of pixels\n",
        "print('Ratio - Each pixel is',pixel_cm_ratio, 'cm*cm')"
      ],
      "metadata": {
        "id": "JtGObRsw9R92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the image\n",
        "def normalize(img):\n",
        "  numerator = img - np.min(img)\n",
        "  denominator = np.max(img) - np.min(img)\n",
        "  return numerator / denominator\n",
        "\n",
        "# Calculating the MGVRI of a given image:\n",
        "def calc_mgvri(img):\n",
        "  img = normalize(img)\n",
        "  green = img[:,:,1] # Getting the green channel\n",
        "  red = img[:,:,0] # Getting the red channel\n",
        "  numerator = np.subtract(green*green, red*red)\n",
        "  denominator = np.add(green*green, red*red)\n",
        "  np.seterr(invalid='ignore') # ignore 0/0 when dividing\n",
        "  MGVRI = np.divide(numerator,denominator) # calculating MGVRI according to the formula\n",
        "  return MGVRI"
      ],
      "metadata": {
        "id": "3XVEXOFe99mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mgvried = calc_mgvri(copy)\n",
        "fig2 = plt.figure(figsize=(7,7)) # Setting the figure size\n",
        "plt.axis('off') # Turning the axis off\n",
        "\n",
        "mask = mgvried < 0.2 # Setting a threshold\n",
        "copy[mask] = -1 # Setting a value of every pixel that didnt make the cut to -1, so that it will be colored white-ish.\n",
        "plt.imshow(copy, cmap=\"Greens\") # Displaying the masked image.\n",
        "\n",
        "ticks = [-1.0, -0.5, 0, 0.5, 1.0] # Setting tick values\n",
        "cb = plt.colorbar(\n",
        "      orientation = \"horizontal\",\n",
        "      ticks = ticks,\n",
        "        pad = 0) # Changing the location of the colorbar\n",
        "cb.set_label('MGVRI') # Adding a label to the colorbar"
      ],
      "metadata": {
        "id": "sdTBrpKq-CVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#counting green pixels\n",
        "green_counter = len(copy[mask])\n",
        "print(\"There are\",green_counter,\"green pixels\\n\")\n",
        "\n",
        "#calculating the aarea both in cm*cm and m*m\n",
        "area = green_counter*pixel_cm_ratio\n",
        "print(\"Area in cm\\N{SUPERSCRIPT TWO}:\",area)\n",
        "print(\"Area in m\\N{SUPERSCRIPT TWO}:\",area/10000)"
      ],
      "metadata": {
        "id": "JXQz4lr_-IYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML"
      ],
      "metadata": {
        "id": "MEbxiapG-USS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tensorflow"
      ],
      "metadata": {
        "id": "b-J-H7o-QSY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions for loading image (hidden)\n",
        "\n",
        "original_image_cache = {}\n",
        "\n",
        "def preprocess_image(image):\n",
        "  image = np.array(image)\n",
        "  # reshape into shape [batch_size, height, width, num_channels]\n",
        "  img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)\n",
        "  return image\n",
        "\n",
        "def load_image_from_url(img_url):\n",
        "  \"\"\"Returns an image with shape [1, height, width, num_channels].\"\"\"\n",
        "  user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}\n",
        "  response = requests.get(img_url, headers=user_agent)\n",
        "  image = Image.open(BytesIO(response.content))\n",
        "  image = preprocess_image(image)\n",
        "  return image\n",
        "\n",
        "def load_image(image_url, image_size=256, dynamic_size=False, max_dynamic_size=512):\n",
        "  \"\"\"Loads and preprocesses images.\"\"\"\n",
        "  # Cache image file locally.\n",
        "  if image_url in original_image_cache:\n",
        "    img = original_image_cache[image_url]\n",
        "  elif image_url.startswith('https://'):\n",
        "    img = load_image_from_url(image_url)\n",
        "  else:\n",
        "    fd = tf.io.gfile.GFile(image_url, 'rb')\n",
        "    img = preprocess_image(Image.open(fd))\n",
        "  original_image_cache[image_url] = img\n",
        "  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n",
        "  img_raw = img\n",
        "  if tf.reduce_max(img) > 1.0:\n",
        "    img = img / 255.\n",
        "  if len(img.shape) == 3:\n",
        "    img = tf.stack([img, img, img], axis=-1)\n",
        "  if not dynamic_size:\n",
        "    img = tf.image.resize_with_pad(img, image_size, image_size)\n",
        "  elif img.shape[1] > max_dynamic_size or img.shape[2] > max_dynamic_size:\n",
        "    img = tf.image.resize_with_pad(img, max_dynamic_size, max_dynamic_size)\n",
        "  return img, img_raw\n",
        "\n",
        "def show_image(image, title=''):\n",
        "  image_size = image.shape[1]\n",
        "  w = (image_size * 6) // 320\n",
        "  plt.figure(figsize=(w, w))\n",
        "  plt.imshow(image[0], aspect='equal')\n",
        "  plt.axis('off')\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "05wxbRmO-lYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select an Image Classification model\n",
        "\n",
        "image_size = 224\n",
        "dynamic_size = False\n",
        "\n",
        "model_name = \"resnet_v2_152\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/classification/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/classification/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/classification/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/classification/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/classification/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/classification/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/classification/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/classification/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/classification/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/classification/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/classification/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/classification/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/classification/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/classification/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/classification/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/classification/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/classification/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/classification/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/classification/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/classification/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/classification/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/ilsvrc2012_classification/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/classification/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/classification/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/classification/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/classification/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/classification/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/classification/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/classification/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/classification/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"mobilenet_v2_100_224\": 224,\n",
        "  \"mobilenet_v2_130_224\": 224,\n",
        "  \"mobilenet_v2_140_224\": 224,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"nasnet_mobile\": 224,\n",
        "  \"pnasnet_large\": 331,\n",
        "  \"resnet_v1_50\": 224,\n",
        "  \"resnet_v1_101\": 224,\n",
        "  \"resnet_v1_152\": 224,\n",
        "  \"resnet_v2_50\": 224,\n",
        "  \"resnet_v2_101\": 224,\n",
        "  \"resnet_v2_152\": 224,\n",
        "  \"mobilenet_v3_small_100_224\": 224,\n",
        "  \"mobilenet_v3_small_075_224\": 224,\n",
        "  \"mobilenet_v3_large_100_224\": 224,\n",
        "  \"mobilenet_v3_large_075_224\": 224,\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map[model_name]\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "\n",
        "max_dynamic_size = 512\n",
        "if model_name in model_image_size_map:\n",
        "  image_size = model_image_size_map[model_name]\n",
        "  dynamic_size = False\n",
        "  print(f\"Images will be converted to {image_size}x{image_size}\")\n",
        "else:\n",
        "  dynamic_size = True\n",
        "  print(f\"Images will be capped to a max size of {max_dynamic_size}x{max_dynamic_size}\")\n",
        "\n",
        "labels_file = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n",
        "\n",
        "#download labels and creates a maps\n",
        "downloaded_file = tf.keras.utils.get_file(\"labels.txt\", origin=labels_file)\n",
        "\n",
        "classes = []\n",
        "\n",
        "with open(downloaded_file) as f:\n",
        "  labels = f.readlines()\n",
        "  classes = [l.strip() for l in labels]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y08dnwMZQW-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select an Input Image\n",
        "\n",
        "image_name = \"flamingo\" # @param ['tiger', 'bus', 'car', 'cat', 'dog', 'apple', 'banana', 'turtle', 'flamingo', 'piano', 'honeycomb', 'teapot']\n",
        "\n",
        "images_for_test_map = {\n",
        "    \"tiger\": \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Bengal_tiger_%28Panthera_tigris_tigris%29_female_3_crop.jpg\",\n",
        "    #by Charles James Sharp, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons\n",
        "    \"bus\": \"https://upload.wikimedia.org/wikipedia/commons/6/63/LT_471_%28LTZ_1471%29_Arriva_London_New_Routemaster_%2819522859218%29.jpg\",\n",
        "    #by Martin49 from London, England, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons\n",
        "    \"car\": \"https://upload.wikimedia.org/wikipedia/commons/4/49/2013-2016_Toyota_Corolla_%28ZRE172R%29_SX_sedan_%282018-09-17%29_01.jpg\",\n",
        "    #by EurovisionNim, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons\n",
        "    \"cat\": \"https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg\",\n",
        "    #by Alvesgaspar, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n",
        "    \"dog\": \"https://upload.wikimedia.org/wikipedia/commons/archive/a/a9/20090914031557%21Saluki_dog_breed.jpg\",\n",
        "    #by Craig Pemberton, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n",
        "    \"apple\": \"https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\",\n",
        "    #by Abhijit Tembhekar from Mumbai, India, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons\n",
        "    \"banana\": \"https://upload.wikimedia.org/wikipedia/commons/1/1c/Bananas_white_background.jpg\",\n",
        "    #by fir0002  flagstaffotos [at] gmail.com\t\tCanon 20D + Tamron 28-75mm f/2.8, GFDL 1.2 <http://www.gnu.org/licenses/old-licenses/fdl-1.2.html>, via Wikimedia Commons\n",
        "    \"turtle\": \"https://upload.wikimedia.org/wikipedia/commons/8/80/Turtle_golfina_escobilla_oaxaca_mexico_claudio_giovenzana_2010.jpg\",\n",
        "    #by Claudio Giovenzana, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n",
        "    \"flamingo\": \"https://upload.wikimedia.org/wikipedia/commons/b/b8/James_Flamingos_MC.jpg\",\n",
        "    #by Christian Mehlfhrer, User:Chmehl, CC BY 3.0 <https://creativecommons.org/licenses/by/3.0>, via Wikimedia Commons\n",
        "    \"piano\": \"https://upload.wikimedia.org/wikipedia/commons/d/da/Steinway_%26_Sons_upright_piano%2C_model_K-132%2C_manufactured_at_Steinway%27s_factory_in_Hamburg%2C_Germany.png\",\n",
        "    #by \"Photo:  Copyright Steinway & Sons\", CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n",
        "    \"honeycomb\": \"https://upload.wikimedia.org/wikipedia/commons/f/f7/Honey_comb.jpg\",\n",
        "    #by Merdal, CC BY-SA 3.0 <http://creativecommons.org/licenses/by-sa/3.0/>, via Wikimedia Commons\n",
        "    \"teapot\": \"https://upload.wikimedia.org/wikipedia/commons/4/44/Black_tea_pot_cropped.jpg\",\n",
        "    #by Mendhak, CC BY-SA 2.0 <https://creativecommons.org/licenses/by-sa/2.0>, via Wikimedia Commons\n",
        "}\n",
        "\n",
        "img_url = images_for_test_map[image_name]\n",
        "image, original_image = load_image(img_url, image_size, dynamic_size, max_dynamic_size)\n",
        "show_image(image, 'Scaled image')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ixzPICCUQz60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model with tf and inserting an initial input\n",
        "classifier = hub.load(model_handle)\n",
        "\n",
        "input_shape = image.shape\n",
        "warmup_input = tf.random.uniform(input_shape, 0, 1.0)\n",
        "%time warmup_logits = classifier(warmup_input).numpy()"
      ],
      "metadata": {
        "id": "M_mUjoXQQ_0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model on image, creating the top 5 classifications and its probabilities\n",
        "%time probabilities = tf.nn.softmax(classifier(image)).numpy()\n",
        "\n",
        "top_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy()\n",
        "np_classes = np.array(classes)\n",
        "\n",
        "# Some models include an additional 'background' class in the predictions, so\n",
        "# we must account for this when reading the class labels.\n",
        "includes_background_class = probabilities.shape[1] == 1001\n",
        "\n",
        "for i, item in enumerate(top_5):\n",
        "  class_index = item if includes_background_class else item + 1\n",
        "  line = f'({i+1}) {class_index:4} - {classes[class_index]}: {probabilities[0][top_5][i]}'\n",
        "  print(line)\n",
        "\n",
        "show_image(image, '')"
      ],
      "metadata": {
        "id": "mzdPBzhBRSDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to switch models, you will need to restart the runtime\n",
        "\n",
        "# instll the dependencies\n",
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies"
      ],
      "metadata": {
        "id": "0GE3FXQWSG1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ],
      "metadata": {
        "id": "MH6tqhH3SPKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# libs\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io\n",
        "\n",
        "# upload an image - you can try 'citrus.jpg' from /Class10/images\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "img = io.imread(img_path)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "iFMSq5bbSiJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of links to images\n",
        "#imgs = ['https://ultralytics.com/images/zidane.jpg']  # batch of images\n",
        "\n",
        "# Inference\n",
        "results = model(img) # accepts URL, Filename, PIL, OpenCV, Numpy..\n",
        "\n",
        "# Results\n",
        "#results.print()\n",
        "results.show()  # or .save()\n",
        "\n",
        "results.xyxy[0]  # img1 predictions (tensor)\n",
        "results.pandas().xyxy[0]  # img1 predictions (pandas)\n",
        "\n"
      ],
      "metadata": {
        "id": "vffcpIJbSjJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFSzsLeJSzfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mask RCNN"
      ],
      "metadata": {
        "id": "nvqb_WWQTILi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Some things that needs to be done before:"
      ],
      "metadata": {
        "id": "lE7iA-rVTerH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "KPn7FnUbTMDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb\n",
        "!dpkg -i libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb\n",
        "!ls -l /usr/lib/x86_64-linux-gnu/libcudnn.so.*\n",
        "!pip install -U -qq tensorflow==2.5.0\n",
        "exit() # Runtime restart required!"
      ],
      "metadata": {
        "id": "AqU55ioHTbmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch rename files script\n",
        "# use it, if you downloaded a lot of images from google, and you want to rename them from img0 to img100 for example\n",
        "\n",
        "# Python 3 code to rename multiple\n",
        "# files in a directory or folder\n",
        "\n",
        "# importing os module\n",
        "import os\n",
        "\n",
        "# Function to rename multiple files\n",
        "def main():\n",
        "\n",
        "\tfolder = \"/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn/dataset/test\"\n",
        "\tfor count, filename in enumerate(os.listdir(folder)):\n",
        "\t\tdst = f\"image{str(count)}.jpg\"\n",
        "\t\tsrc =f\"{folder}/{filename}\" # foldername/filename, if .py file is outside folder\n",
        "\t\tdst =f\"{folder}/{dst}\"\n",
        "\t\t\n",
        "\t\t# rename() function will\n",
        "\t\t# rename all the files\n",
        "\t\tos.rename(src, dst)\n",
        "\n",
        "# Driver Code\n",
        "if __name__ == '__main__':\n",
        "\t\n",
        "\t# Calling main() function\n",
        "\tmain()\n"
      ],
      "metadata": {
        "id": "lhguVefMVKfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning the MASK-RCNN repo\n",
        "!git clone https://github.com/kairess/Mask_RCNN"
      ],
      "metadata": {
        "id": "WGyqtuWxVNHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libs\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "ROOT_DIR = 'Mask_RCNN'\n",
        "\n",
        "sys.path.append(ROOT_DIR) \n",
        "from mrcnn.config import Config\n",
        "import mrcnn.utils as utils\n",
        "from mrcnn import visualize\n",
        "import mrcnn.model as modellib"
      ],
      "metadata": {
        "id": "zExHqIXvVQ0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the pretrained model\n",
        "# This will default to sub-directories in your mask_rcnn_dir, but if you want them somewhere else, updated it here.\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "metadata": {
        "id": "-4NvMwYoVX8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainConfig(Config):\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"custom\"\n",
        "\n",
        "    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 5\n",
        "\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    # Number of classes (including background) - IMPORTANT TO CHANGE ACCORDING TO YOUR LABELS IN YOUR JSON\n",
        "    NUM_CLASSES = 1 + 1  # background + 1 (flowers)\n",
        "\n",
        "    # All of our training images are 1920x1012\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    \n",
        "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
        "    BACKBONE = 'resnet50' # resnet50\n",
        "\n",
        "    # To be honest, I haven't taken the time to figure out what these do\n",
        "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "    MAX_GT_INSTANCES = 50 \n",
        "    POST_NMS_ROIS_INFERENCE = 500 \n",
        "    POST_NMS_ROIS_TRAINING = 1000 \n",
        "    \n",
        "config = TrainConfig()\n",
        "config.display()"
      ],
      "metadata": {
        "id": "nKvZG38jVk1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoLikeDataset(utils.Dataset):\n",
        "    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n",
        "        See http://cocodataset.org/#home for more information.\n",
        "    \"\"\"\n",
        "    def load_data(self, annotation_json, images_dir):\n",
        "        \"\"\" Load the coco-like dataset from json\n",
        "        Args:\n",
        "            annotation_json: The path to the coco annotations json file\n",
        "            images_dir: The directory holding the images referred to by the json file\n",
        "        \"\"\"\n",
        "        # Load json from file\n",
        "        json_file = open(annotation_json)\n",
        "        coco_json = json.load(json_file)\n",
        "        json_file.close()\n",
        "        \n",
        "        # Add the class names using the base method from utils.Dataset\n",
        "        source_name = \"coco_like\"\n",
        "        for category in coco_json['categories']:\n",
        "            class_id = category['id']\n",
        "            class_name = category['name']\n",
        "            if class_id < 1:\n",
        "                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n",
        "                return\n",
        "            \n",
        "            self.add_class(source_name, class_id, class_name)\n",
        "        \n",
        "        # Get all annotations\n",
        "        annotations = {}\n",
        "        for annotation in coco_json['annotations']:\n",
        "            image_id = annotation['image_id']\n",
        "            if image_id not in annotations:\n",
        "                annotations[image_id] = []\n",
        "            annotations[image_id].append(annotation)\n",
        "        \n",
        "        # Get all images and add them to the dataset\n",
        "        seen_images = {}\n",
        "        for image in coco_json['images']:\n",
        "            image_id = image['id']\n",
        "            if image_id in seen_images:\n",
        "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
        "            else:\n",
        "                seen_images[image_id] = image\n",
        "                try:\n",
        "                    image_file_name = image['file_name']\n",
        "                    image_width = image['width']\n",
        "                    image_height = image['height']\n",
        "                except KeyError as key:\n",
        "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
        "                \n",
        "                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n",
        "                image_annotations = annotations[image_id]\n",
        "                \n",
        "                # Add the image using the base method from utils.Dataset\n",
        "                self.add_image(\n",
        "                    source=source_name,\n",
        "                    image_id=image_id,\n",
        "                    path=image_path,\n",
        "                    width=image_width,\n",
        "                    height=image_height,\n",
        "                    annotations=image_annotations\n",
        "                )\n",
        "                \n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\" Load instance masks for the given image.\n",
        "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
        "        Args:\n",
        "            image_id: The id of the image to load masks for\n",
        "        Returns:\n",
        "            masks: A bool array of shape [height, width, instance count] with\n",
        "                one mask per instance.\n",
        "            class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        image_info = self.image_info[image_id]\n",
        "        annotations = image_info['annotations']\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        \n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
        "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
        "            for segmentation in annotation['segmentation']:\n",
        "                mask_draw.polygon(segmentation, fill=1)\n",
        "                bool_array = np.array(mask) > 0\n",
        "                instance_masks.append(bool_array)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        mask = np.dstack(instance_masks)\n",
        "        class_ids = np.array(class_ids, dtype=np.int32)\n",
        "        \n",
        "        return mask, class_ids"
      ],
      "metadata": {
        "id": "JXTouENGVqbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = r'/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn'\n",
        "\n",
        "dataset_train = CocoLikeDataset()\n",
        "dataset_train.load_data(f'{root_folder}/dataset/train.json', f'{root_folder}/dataset/train/')\n",
        "dataset_train.prepare()\n",
        "\n",
        "dataset_val = CocoLikeDataset()\n",
        "dataset_val.load_data(f'{root_folder}/dataset/val.json', f'{root_folder}/dataset/val/')\n",
        "dataset_val.prepare()\n",
        "\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "\n",
        "print('Train', len(dataset_train.image_ids))\n",
        "print('Validation', len(dataset_val.image_ids))\n",
        "\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ],
      "metadata": {
        "id": "6X9UNMYgV49b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.class_names"
      ],
      "metadata": {
        "id": "ctH9S0lZV8fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = modellib.MaskRCNN(\n",
        "    mode=\"training\",\n",
        "    config=config,\n",
        "    model_dir=MODEL_DIR)\n",
        "\n",
        "model.load_weights(\n",
        "    COCO_MODEL_PATH,\n",
        "    by_name=True,\n",
        "    exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ],
      "metadata": {
        "id": "ASfsaKzqV-g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "start_train = time.time()\n",
        "\n",
        "model.train(\n",
        "    dataset_train,\n",
        "    dataset_val, \n",
        "    learning_rate=config.LEARNING_RATE, \n",
        "    epochs=30, \n",
        "    layers='heads')\n",
        "\n",
        "end_train = time.time()\n",
        "minutes = round((end_train - start_train) / 60, 2)\n",
        "\n",
        "print(f'Training took {minutes} minutes')"
      ],
      "metadata": {
        "id": "bSe4IjKdWBY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the trained model to disk\n",
        "\n",
        "import shutil\n",
        "\n",
        "original = r'/content/Mask_RCNN/logs/custom20221228T1436/mask_rcnn_custom_0030.h5'\n",
        "target = r'/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn/mask_rcnn_custom_0030_28122022_flowers.h5'\n",
        "\n",
        "shutil.copyfile(original, target)"
      ],
      "metadata": {
        "id": "P03jg6CUWpFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceConfig(TrainConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    DETECTION_MIN_CONFIDENCE = 0.65 # CHANGE HERE IF YOU WANT\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "test_model = modellib.MaskRCNN(\n",
        "    mode=\"inference\", \n",
        "    config=inference_config,\n",
        "    model_dir=MODEL_DIR)\n",
        "\n",
        "model_path = test_model.find_last()\n",
        "print(model_path)\n",
        "\n",
        "test_model.load_weights(model_path, by_name=True)"
      ],
      "metadata": {
        "id": "e1AUg05dWsfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "\n",
        "mask_colors = [\n",
        "    (0., 0., 0.), # Background\n",
        "    (1., 0., 0.), # Red\n",
        "    (0., 1., 0.)  # Green\n",
        "]\n",
        "\n",
        "real_test_dir = f'{root_folder}/dataset/test'\n",
        "image_paths = []\n",
        "\n",
        "for filename in os.listdir(real_test_dir):\n",
        "    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n",
        "        image_paths.append(os.path.join(real_test_dir, filename))\n",
        "\n",
        "for image_path in image_paths:\n",
        "    img = skimage.io.imread(image_path)\n",
        "    img_arr = np.array(img)\n",
        "\n",
        "    results = test_model.detect([img_arr], verbose=1)\n",
        "    r = results[0]\n",
        "\n",
        "    colors = tuple(np.take(mask_colors, r['class_ids'], axis=0))\n",
        "\n",
        "    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n",
        "                                dataset_val.class_names, r['scores'], figsize=(16, 8),\n",
        "                                colors=colors)"
      ],
      "metadata": {
        "id": "F1P5LjKtWvj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}